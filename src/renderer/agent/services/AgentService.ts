/**
 * Agent æœåŠ¡
 * æ ¸å¿ƒçš„ Agent å¾ªç¯é€»è¾‘ï¼Œå¤„ç† LLM é€šä¿¡å’Œå·¥å…·æ‰§è¡Œ
 */

import { api } from '@/renderer/services/electronAPI'
import { logger } from '@utils/Logger'
import { normalizePath } from '@shared/utils/pathUtils'
import { performanceMonitor, CacheService, withRetry, isRetryableError } from '@shared/utils'
import { AppError, formatErrorMessage } from '@/shared/errors'
import { useAgentStore } from '../store/AgentStore'
import { useStore } from '@store'
import { WorkMode } from '@/renderer/modes/types'
import { toolRegistry, toolManager, initializeToolProviders, setToolLoadingContext } from '../tools'
import { OpenAIMessage } from '../llm/MessageConverter'
import {
  ContextItem,
  MessageContent,
  TextContent,
} from '../types'
import { LLMStreamChunk, LLMToolCall, LLMResult } from '@/renderer/types/electron'
import { getReadOnlyTools } from '@/shared/config/tools'

// å¯¼å…¥æ‹†åˆ†çš„æ¨¡å—
import {
  getAgentConfig,
  READ_TOOLS,
} from '../utils/AgentConfig'
import { LoopDetector } from '../utils/LoopDetector'
import {
  createStreamHandlerState,
  StreamHandlerState,
  handleTextChunk,
  handleReasoningChunk,
  closeReasoningIfNeeded,
  handleToolCallStart,
  handleToolCallDelta,
  handleToolCallEnd,
  handleFullToolCall,
  handleLLMToolCall,
  handleLLMDone,
  detectStreamingXMLToolCalls,
} from '../llm/LLMStreamHandler'
import {
  buildContextContent,
  calculateContextStats,
} from '../llm/ContextBuilder'

// å¯¼å…¥æ–°çš„æœåŠ¡æ¨¡å—
import { toolExecutionService } from './ToolExecutionService'
import { buildLLMMessages, compressContext } from '../llm/MessageBuilder'
import { executeToolCallsIntelligently } from './ParallelToolExecutor'
import { composerService } from './composerService'

// Agent æ–‡ä»¶è¯»å–ç¼“å­˜ï¼ˆå¸¦ LRU æ·˜æ±°ï¼‰
const agentFileCache = new CacheService<string>('AgentFileCache', {
  maxSize: 200,
  maxMemory: 30 * 1024 * 1024, // 30MB
  defaultTTL: 10 * 60 * 1000,  // 10 åˆ†é’Ÿ
})

export interface LLMCallConfig {
  provider: string
  model: string
  apiKey: string
  baseUrl?: string
  timeout?: number
  maxTokens?: number
  temperature?: number
  topP?: number
  adapterConfig?: import('@/shared/config/providers').LLMAdapterConfig
  advanced?: import('@/shared/config/providers').AdvancedConfig
}

// ===== Agent æœåŠ¡ç±» =====

class AgentServiceClass {
  private abortController: AbortController | null = null
  private currentAssistantId: string | null = null
  private isRunning = false
  private unsubscribers: (() => void)[] = []
  private streamState: StreamHandlerState = createStreamHandlerState()
  private throttleState = { lastUpdate: 0, lastArgsLen: 0 }

  /**
   * æ£€æŸ¥æ–‡ä»¶ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
   */
  hasValidFileCache(filePath: string): boolean {
    return agentFileCache.has(normalizePath(filePath))
  }

  /**
   * æ ‡è®°æ–‡ä»¶å·²è¯»å–
   */
  markFileAsRead(filePath: string, content: string): void {
    agentFileCache.set(normalizePath(filePath), this.fnvHash(content))
  }

  /**
   * è·å–æ–‡ä»¶çš„ç¼“å­˜å†…å®¹å“ˆå¸Œ
   */
  getFileCacheHash(filePath: string): string | null {
    return agentFileCache.get(normalizePath(filePath)) ?? null
  }

  /**
   * æ¸…é™¤ä¼šè¯ç¼“å­˜
   */
  clearSession(): void {
    agentFileCache.clear()
    logger.agent.info('[Agent] Session cleared')
  }

  /**
   * è·å–ç¼“å­˜ç»Ÿè®¡
   */
  getCacheStats() {
    return agentFileCache.getStats()
  }

  /**
   * FNV-1a å“ˆå¸Œç®—æ³•
   */
  private fnvHash(str: string): string {
    let h1 = 0x811c9dc5
    let h2 = 0x811c9dc5
    const len = str.length
    const mid = len >> 1

    for (let i = 0; i < mid; i++) {
      h1 ^= str.charCodeAt(i)
      h1 = Math.imul(h1, 0x01000193)
    }
    for (let i = mid; i < len; i++) {
      h2 ^= str.charCodeAt(i)
      h2 = Math.imul(h2, 0x01000193)
    }

    return ((h1 >>> 0).toString(36) + (h2 >>> 0).toString(36))
  }

  async calculateContextStats(contextItems: ContextItem[], currentInput: string): Promise<void> {
    return calculateContextStats(contextItems, currentInput)
  }

  // ===== å…¬å…±æ–¹æ³• =====

  async sendMessage(
    userMessage: MessageContent,
    config: LLMCallConfig,
    workspacePath: string | null,
    systemPrompt: string,
    chatMode: WorkMode = 'agent'
  ): Promise<void> {
    if (this.isRunning) {
      logger.agent.warn('[Agent] Already running, ignoring new request')
      return
    }

    const store = useAgentStore.getState()

    if (!config.apiKey) {
      this.showError('Please configure your API key in settings.')
      return
    }

    this.isRunning = true
    this.abortController = new AbortController()

    try {
      const contextItems = store.getCurrentThread()?.contextItems || []
      const userQuery = typeof userMessage === 'string' ? userMessage :
        (Array.isArray(userMessage) ? userMessage.filter(p => p.type === 'text').map(p => (p as TextContent).text).join('') : '')

      const contextContent = await buildContextContent(contextItems, userQuery)
      const userMessageId = store.addUserMessage(userMessage, contextItems)
      store.clearContextItems()

      const messageText = typeof userMessage === 'string'
        ? userMessage.slice(0, 50)
        : 'User message'
      await store.createMessageCheckpoint(userMessageId, messageText)

      const llmMessages = await buildLLMMessages(userMessage, contextContent, systemPrompt)
      this.currentAssistantId = store.addAssistantMessage()
      store.setStreamPhase('streaming')

      // å¯åŠ¨ Composer Session ç”¨äºå¤šæ–‡ä»¶å˜æ›´è¿½è¸ª
      composerService.startSession(
        userQuery.slice(0, 50) || 'Agent Task',
        `Started at ${new Date().toLocaleTimeString()}`
      )

      await this.runAgentLoop(config, llmMessages, workspacePath, chatMode)
    } catch (error) {
      const appError = AppError.fromError(error)
      logger.agent.error('[Agent] Error:', appError.toJSON())
      this.showError(formatErrorMessage(appError))
    } finally {
      this.cleanup()
    }
  }

  // å§”æ‰˜ç»™ ToolExecutionService å¤„ç†å®¡æ‰¹
  approve(): void {
    toolExecutionService.approve()
  }

  reject(): void {
    toolExecutionService.reject()
  }



  abort(): void {
    if (this.abortController) {
      this.abortController.abort()
    }
    api.llm.abort()

    // é€šçŸ¥ ToolExecutionService æ‹’ç»å½“å‰ç­‰å¾…çš„å®¡æ‰¹
    toolExecutionService.reject()

    const store = useAgentStore.getState()
    
    // ç¡®ä¿å½“å‰åŠ©æ‰‹æ¶ˆæ¯çš„å·¥å…·è°ƒç”¨çŠ¶æ€è¢«æ›´æ–°
    if (this.currentAssistantId) {
      const thread = store.getCurrentThread()
      if (thread) {
        const assistantMsg = thread.messages.find(
          m => m.id === this.currentAssistantId && m.role === 'assistant'
        )
        if (assistantMsg && assistantMsg.role === 'assistant') {
          for (const tc of (assistantMsg as any).toolCalls || []) {
            if (['running', 'awaiting', 'pending'].includes(tc.status)) {
              store.updateToolCall(this.currentAssistantId, tc.id, {
                status: 'error',
                error: 'Aborted by user',
              })
            }
          }
        }
      }
      
      // ç¡®ä¿æ¶ˆæ¯çš„ isStreaming è¢«è®¾ç½®ä¸º false
      store.finalizeAssistant(this.currentAssistantId)
    }
    
    // é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰æ­£åœ¨æµå¼è¾“å‡ºçš„æ¶ˆæ¯éƒ½è¢«ç»ˆæ­¢
    const thread = store.getCurrentThread()
    if (thread) {
      for (const msg of thread.messages) {
        if (msg.role === 'assistant' && (msg as any).isStreaming) {
          store.finalizeAssistant(msg.id)
        }
      }
    }

    this.cleanup()
  }

  // ===== ç§æœ‰æ–¹æ³•ï¼šæ ¸å¿ƒé€»è¾‘ =====

  private async runAgentLoop(
    config: LLMCallConfig,
    llmMessages: OpenAIMessage[],
    workspacePath: string | null,
    chatMode: WorkMode
  ): Promise<void> {
    const store = useAgentStore.getState()
    let loopCount = 0
    let shouldContinue = true

    // å¢å¼ºçš„å¾ªç¯æ£€æµ‹å™¨
    const loopDetector = new LoopDetector()

    const agentLoopConfig = getAgentConfig()

    while (shouldContinue && loopCount < agentLoopConfig.maxToolLoops && !this.abortController?.signal.aborted) {
      loopCount++
      shouldContinue = false

      logger.agent.info(`[Agent] Loop iteration ${loopCount}`)

      // ä½¿ç”¨ MessageBuilder çš„ compressContext
      await compressContext(llmMessages, agentLoopConfig.contextCompressThreshold)

      const result = await this.callLLMWithRetry(config, llmMessages, chatMode)

      if (this.abortController?.signal.aborted) break

      if (result.error) {
        store.appendToAssistant(this.currentAssistantId!, `\n\nâŒ Error: ${result.error}`)
        break
      }

      // æ³¨æ„ï¼šæ¶ˆæ¯å†…å®¹çš„æ›´æ–°å·²åœ¨ handleLLMDone ä¸­å¤„ç†ï¼ˆåŒ…æ‹¬ XML å·¥å…·è°ƒç”¨æ¸…ç†ï¼‰
      // è¿™é‡Œä¸å†é‡å¤æ›´æ–°ï¼Œé¿å…å†…å®¹ä¸ä¸€è‡´

      if (!result.toolCalls || result.toolCalls.length === 0) {
        // åªæœ‰åœ¨ plan æ¨¡å¼ä¸‹æ‰æé†’æ›´æ–° plan
        if (chatMode === 'plan' && store.plan) {
          const readOnlyTools = getReadOnlyTools()
          const hasWriteOps = llmMessages.some(m => m.role === 'assistant' && m.tool_calls?.some((tc: any) => !readOnlyTools.includes(tc.function.name)))
          const hasUpdatePlan = llmMessages.some(m => m.role === 'assistant' && m.tool_calls?.some((tc: any) => tc.function.name === 'update_plan'))

          if (hasWriteOps && !hasUpdatePlan && loopCount < agentLoopConfig.maxToolLoops) {
            logger.agent.info('[Agent] Plan mode detected: Reminding AI to update plan status')
            llmMessages.push({
              role: 'user' as const,
              content: 'Reminder: You have performed some actions. Please use `update_plan` to update the plan status (e.g., mark the current step as completed) before finishing your response.',
            })
            shouldContinue = true
            continue
          }
        }

        logger.agent.info('[Agent] No tool calls, task complete')
        break
      }

      // ä½¿ç”¨å¢å¼ºçš„å¾ªç¯æ£€æµ‹
      const loopResult = loopDetector.checkLoop(result.toolCalls)
      if (loopResult.isLoop) {
        logger.agent.warn(`[Agent] Loop detected: ${loopResult.reason}`)
        const suggestion = loopResult.suggestion ? `\nğŸ’¡ ${loopResult.suggestion}` : ''
        store.appendToAssistant(this.currentAssistantId!, `\n\nâš ï¸ ${loopResult.reason}${suggestion}`)
        break
      }

      if (this.currentAssistantId) {
        const currentMsg = store.getMessages().find(m => m.id === this.currentAssistantId)
        if (currentMsg && currentMsg.role === 'assistant') {
          const existingToolCalls = (currentMsg as any).toolCalls || []

          for (const tc of result.toolCalls) {
            const existing = existingToolCalls.find((e: any) => e.id === tc.id)
            if (!existing) {
              store.addToolCallPart(this.currentAssistantId, {
                id: tc.id,
                name: tc.name,
                arguments: tc.arguments,
              })
            } else if (!existing.status) {
              store.updateToolCall(this.currentAssistantId, tc.id, { status: 'pending' })
            }
          }
        }
      }

      llmMessages.push({
        role: 'assistant',
        content: result.content || null,
        tool_calls: result.toolCalls.map(tc => ({
          id: tc.id,
          type: 'function' as const,
          function: {
            name: tc.name,
            arguments: JSON.stringify(tc.arguments),
          },
        })),
      })

      let userRejected = false

      logger.agent.info(`[Agent] Executing ${result.toolCalls.length} tool calls intelligently`)

      // ä½¿ç”¨æ™ºèƒ½å¹¶è¡Œæ‰§è¡Œå™¨
      const { results: toolResults, userRejected: rejected } = await executeToolCallsIntelligently(
        result.toolCalls,
        {
          workspacePath,
          currentAssistantId: this.currentAssistantId,
        },
        this.abortController?.signal
      )

      userRejected = rejected

      // å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
      for (const { toolCall, result: toolResult } of toolResults) {
        llmMessages.push({
          role: 'tool' as const,
          tool_call_id: toolCall.id,
          content: toolResult.content,
        })

        // æ›´æ–° LoopDetector çš„å†…å®¹å“ˆå¸Œï¼ˆç”¨äºæ£€æµ‹æ–‡ä»¶å†…å®¹æ˜¯å¦çœŸæ­£å˜åŒ–ï¼‰
        const meta = toolResult.meta
        if (meta?.filePath && typeof meta.filePath === 'string' && typeof meta.newContent === 'string') {
          loopDetector.updateContentHash(meta.filePath, meta.newContent)
        }
      }

      // æ”¶é›†å†™æ“ä½œç”¨äºè‡ªåŠ¨æ£€æŸ¥
      const writeToolCalls = result.toolCalls.filter(tc => !READ_TOOLS.includes(tc.name))

      const { agentConfig } = useStore.getState()
      if (agentConfig.enableAutoFix && !userRejected && writeToolCalls.length > 0 && workspacePath) {
        const observation = await this.observeChanges(workspacePath, writeToolCalls)
        if (observation.hasErrors && observation.errors.length > 0) {
          const observeMessage = `[Observation] æ£€æµ‹åˆ°ä»¥ä¸‹ä»£ç é—®é¢˜ï¼Œè¯·ä¿®å¤ï¼š\n\n${observation.errors.slice(0, 3).join('\n\n')}`
          llmMessages.push({
            role: 'user' as const,
            content: observeMessage,
          })
          store.appendToAssistant(this.currentAssistantId!, `\n\nğŸ” **Auto-check**: Detected ${observation.errors.length} issue(s). Attempting to fix...`)
        }
      }

      // æ£€æŸ¥æ˜¯å¦æ˜¾ç¤ºå®‰å…¨è­¦å‘Š
      const { securitySettings } = useStore.getState()
      if (securitySettings.showSecurityWarnings !== false) {
        const recentMessages = store.getMessages()
        const hasWhitelistError = recentMessages.some(msg =>
          msg.role === 'tool' && (msg.content.includes('whitelist') || msg.content.includes('ç™½åå•'))
        )
        if (hasWhitelistError) {
          store.appendToAssistant(this.currentAssistantId!, '\n\nğŸ’¡ **Tip**: You can add commands to the whitelist in Settings > Security > Shell Command Whitelist.')
        }
      }

      if (userRejected) break

      shouldContinue = true
      store.setStreamPhase('streaming')
    }

    if (loopCount >= agentLoopConfig.maxToolLoops) {
      store.appendToAssistant(this.currentAssistantId!, '\n\nâš ï¸ Reached maximum tool call limit.')
    }
  }

  private async callLLMWithRetry(
    config: LLMCallConfig,
    messages: OpenAIMessage[],
    chatMode: WorkMode
  ): Promise<{ content?: string; toolCalls?: LLMToolCall[]; error?: string }> {
    const retryConfig = getAgentConfig()

    try {
      return await withRetry(
        async () => {
          if (this.abortController?.signal.aborted) {
            throw new Error('Aborted')
          }
          const result = await this.callLLM(config, messages, chatMode)
          if (result.error) {
            throw new Error(result.error)
          }
          return result
        },
        {
          maxRetries: retryConfig.maxRetries,
          initialDelayMs: retryConfig.retryDelayMs,
          backoffMultiplier: retryConfig.retryBackoffMultiplier,
          isRetryable: (error) => {
            const message = error instanceof Error ? error.message : String(error)
            return isRetryableError(error) || message === 'Aborted' === false
          },
          onRetry: (attempt, error, delay) => {
            logger.agent.info(`[Agent] LLM call failed (attempt ${attempt}), retrying in ${delay}ms...`, error)
          },
        }
      )
    } catch (error) {
      const message = error instanceof Error ? error.message : String(error)
      return { error: message }
    }
  }

  private async callLLM(
    config: LLMCallConfig,
    messages: OpenAIMessage[],
    chatMode: WorkMode
  ): Promise<{ content?: string; toolCalls?: LLMToolCall[]; reasoning?: string; reasoningStartTime?: number; usage?: { promptTokens: number; completionTokens: number; totalTokens: number }; error?: string }> {
    // å¼€å§‹æ€§èƒ½ç›‘æ§
    performanceMonitor.start(`llm:${config.model}`, 'llm', {
      provider: config.provider,
      messageCount: messages.length,
    })

    return new Promise((resolve) => {
      // é‡ç½®æµå¼çŠ¶æ€
      this.streamState = createStreamHandlerState()
      this.throttleState = { lastUpdate: 0, lastArgsLen: 0 }

      const cleanupListeners = () => {
        this.unsubscribers.forEach(unsub => unsub())
        this.unsubscribers = []
      }

      // ç›‘å¬æµå¼æ–‡æœ¬
      this.unsubscribers.push(
        api.llm.onStream((chunk: LLMStreamChunk) => {
          // å¦‚æœæ­£åœ¨æ¨ç†ä½†æ”¶åˆ°éæ¨ç†å†…å®¹ï¼Œå…³é—­æ¨ç†æ ‡ç­¾
          if (this.streamState.isReasoning && chunk.type !== 'reasoning') {
            closeReasoningIfNeeded(this.streamState, this.currentAssistantId)
          }

          // å¤„ç†å„ç±»æµå¼äº‹ä»¶
          handleTextChunk(chunk, this.streamState, this.currentAssistantId)
          
          if (chunk.type === 'text' && this.currentAssistantId) {
            detectStreamingXMLToolCalls(this.streamState, this.currentAssistantId)
          }

          handleReasoningChunk(chunk, this.streamState, this.currentAssistantId)
          handleToolCallStart(chunk, this.streamState, this.currentAssistantId)
          handleToolCallDelta(chunk, this.streamState, this.currentAssistantId, this.throttleState)
          handleToolCallEnd(chunk, this.streamState, this.currentAssistantId)
          handleFullToolCall(chunk, this.streamState, this.currentAssistantId)
        })
      )

      // ç›‘å¬éæµå¼å·¥å…·è°ƒç”¨
      this.unsubscribers.push(
        api.llm.onToolCall((toolCall: LLMToolCall) => {
          handleLLMToolCall(toolCall, this.streamState, this.currentAssistantId)
        })
      )

      // ç›‘å¬å®Œæˆ
      this.unsubscribers.push(
        api.llm.onDone((result: LLMResult) => {
          // ç»“æŸæ€§èƒ½ç›‘æ§
          performanceMonitor.end(`llm:${config.model}`, true)

          cleanupListeners()
          const finalResult = handleLLMDone(result, this.streamState, this.currentAssistantId)
          // æ›´æ–° store ä¸­çš„ usage ä¿¡æ¯
          if (this.currentAssistantId && finalResult.usage) {
            useAgentStore.getState().updateMessage(this.currentAssistantId, {
              usage: finalResult.usage,
            } as any)
          }
          resolve(finalResult)
        })
      )

      // ç›‘å¬é”™è¯¯
      this.unsubscribers.push(
        api.llm.onError((error: { message: string }) => {
          // ç»“æŸæ€§èƒ½ç›‘æ§ï¼ˆå¤±è´¥ï¼‰
          performanceMonitor.end(`llm:${config.model}`, false, { error: error.message })

          closeReasoningIfNeeded(this.streamState, this.currentAssistantId)
          cleanupListeners()
          resolve({ error: error.message })
        })
      )

      // å‘é€è¯·æ±‚
      // åˆå§‹åŒ–å·¥å…·æä¾›è€…å¹¶è·å–æ‰€æœ‰å·¥å…·å®šä¹‰
      initializeToolProviders()
      
      // è®¾ç½®å·¥å…·åŠ è½½ä¸Šä¸‹æ–‡ï¼ˆæ ¹æ®æ¨¡å¼å’Œè§’è‰²åŠ è½½ä¸åŒå·¥å…·ï¼‰
      const templateId = useStore.getState().promptTemplateId
      setToolLoadingContext({
        mode: chatMode === 'plan' ? 'plan' : chatMode === 'chat' ? 'chat' : 'code',
        templateId,
      })
      
      const allTools = chatMode === 'chat' ? [] : toolManager.getAllToolDefinitions()
      
      api.llm.send({
        config,
        messages: messages as any,
        tools: allTools,
        systemPrompt: '',
      }).catch((err) => {
        cleanupListeners()
        resolve({ error: err.message || 'Failed to send message' })
      })
    })
  }


  private showError(message: string): void {
    const store = useAgentStore.getState()
    const id = store.addAssistantMessage()
    store.appendToAssistant(id, `âŒ ${message}`)
    store.finalizeAssistant(id)
  }

  private cleanup(): void {
    this.unsubscribers.forEach(unsub => unsub())
    this.unsubscribers = []

    const store = useAgentStore.getState()
    if (this.currentAssistantId) store.finalizeAssistant(this.currentAssistantId)
    store.setStreamPhase('idle')
    this.currentAssistantId = null
    this.abortController = null
    this.isRunning = false
    this.streamState = createStreamHandlerState()
    
    // å®Œæˆ Composer Sessionï¼ˆä½†ä¸è‡ªåŠ¨å…³é—­ï¼Œè®©ç”¨æˆ·å†³å®šæ˜¯å¦æ¥å—å˜æ›´ï¼‰
    const composerState = composerService.getState()
    if (composerState.currentSession) {
      // æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„å˜æ›´
      const hasPending = composerState.currentSession.changes.some(c => c.status === 'pending')
      if (!hasPending) {
        // å¦‚æœæ²¡æœ‰å¾…å¤„ç†çš„å˜æ›´ï¼Œå®Œæˆ session
        composerService.completeSession()
      }
      // å¦‚æœæœ‰å¾…å¤„ç†çš„å˜æ›´ï¼Œä¿æŒ session æ‰“å¼€ï¼Œè®©ç”¨æˆ·åœ¨ UI ä¸­å¤„ç†
    }
  }

  private async observeChanges(
    workspacePath: string,
    writeToolCalls: LLMToolCall[]
  ): Promise<{ hasErrors: boolean; errors: string[] }> {
    const errors: string[] = []
    const editedFiles = writeToolCalls
      .filter(tc => ['edit_file', 'write_file', 'create_file_or_folder'].includes(tc.name))
      .map(tc => {
        const filePath = tc.arguments.path as string
        return filePath.startsWith(workspacePath) ? filePath : `${workspacePath}/${filePath}`.replace(/\/+/g, '/')
      })
      .filter(path => !path.endsWith('/'))

    for (const filePath of editedFiles) {
      try {
        const lintResult = await toolRegistry.execute('get_lint_errors', { path: filePath }, { workspacePath })
        if (lintResult.success && lintResult.result) {
          const result = lintResult.result.trim()
          if (result && result !== '[]' && result !== 'No diagnostics found') {
            const hasActualError = /\[error\]/i.test(result) ||
              result.toLowerCase().includes('failed to compile') ||
              result.toLowerCase().includes('syntax error')

            if (hasActualError) {
              errors.push(`File: ${filePath}\n${result}`)
            }
          }
        }
      } catch (e) { }
    }
    return { hasErrors: errors.length > 0, errors }
  }
}

export const AgentService = new AgentServiceClass()
