/**
 * Agent ä¸»å¾ªç¯
 * 
 * èŒè´£ï¼š
 * - ç®¡ç† LLM è°ƒç”¨å¾ªç¯
 * - åŸºäºçœŸå® token ä½¿ç”¨é‡çš„ä¸Šä¸‹æ–‡å‹ç¼©
 * - å·¥å…·æ‰§è¡Œåè°ƒ
 * - å¾ªç¯æ£€æµ‹
 * - å‘å¸ƒäº‹ä»¶åˆ° EventBus
 */

import { api } from '@/renderer/services/electronAPI'
import { logger } from '@utils/Logger'
import { performanceMonitor, withRetry, isRetryableError } from '@shared/utils'
import { useAgentStore } from '../store/AgentStore'
import { useStore } from '@store'
import { toolManager, initializeToolProviders, setToolLoadingContext, initializeTools } from '../tools'
import { toolRegistry } from '../tools/registry'
import { getAgentConfig, READ_TOOLS } from '../utils/AgentConfig'
import { LoopDetector } from '../utils/LoopDetector'
import { getReadOnlyTools, isFileEditTool } from '@/shared/config/tools'
import { pathStartsWith, joinPath } from '@shared/utils/pathUtils'
import { createStreamProcessor } from './stream'
import { executeTools } from './tools'
import { EventBus } from './EventBus'
import { 
  generateSummary,
  generateHandoffDocument,
} from '../context'
import { updateStats, LEVEL_NAMES } from '../context/CompressionManager'
import type { OpenAIMessage } from '../llm/MessageConverter'
import type { WorkMode } from '@/renderer/modes/types'
import type { LLMConfig, LLMCallResult, ExecutionContext } from './types'

// ===== LLM è°ƒç”¨ =====

async function callLLM(
  config: LLMConfig,
  messages: OpenAIMessage[],
  chatMode: WorkMode,
  assistantId: string | null
): Promise<LLMCallResult> {
  performanceMonitor.start(`llm:${config.model}`, 'llm', { provider: config.provider, messageCount: messages.length })

  const processor = createStreamProcessor(assistantId)

  // åˆå§‹åŒ–å·¥å…·
  initializeToolProviders()
  await initializeTools()
  const templateId = useStore.getState().promptTemplateId
  setToolLoadingContext({
    mode: chatMode,
    templateId,
  })
  const tools = chatMode === 'chat' ? [] : toolManager.getAllToolDefinitions()

  // å‘é€è¯·æ±‚
  api.llm.send({ config: config as any, messages: messages as any, tools, systemPrompt: '' }).catch(() => {
    processor.cleanup()
  })

  const result = await processor.wait()
  performanceMonitor.end(`llm:${config.model}`, !result.error)

  // æ›´æ–° usage
  if (assistantId && result.usage) {
    useAgentStore.getState().updateMessage(assistantId, { usage: result.usage } as any)
  }

  return result
}

async function callLLMWithRetry(
  config: LLMConfig,
  messages: OpenAIMessage[],
  chatMode: WorkMode,
  assistantId: string | null,
  abortSignal?: AbortSignal
): Promise<LLMCallResult> {
  const retryConfig = getAgentConfig()
  try {
    return await withRetry(
      async () => {
        if (abortSignal?.aborted) throw new Error('Aborted')
        const result = await callLLM(config, messages, chatMode, assistantId)
        if (result.error) throw new Error(result.error)
        return result
      },
      {
        maxRetries: retryConfig.maxRetries,
        initialDelayMs: retryConfig.retryDelayMs,
        backoffMultiplier: retryConfig.retryBackoffMultiplier,
        isRetryable: error => {
          const msg = error instanceof Error ? error.message : String(error)
          return isRetryableError(error) && msg !== 'Aborted'
        },
        onRetry: (attempt, error, delay) => logger.agent.info(`[Loop] LLM retry ${attempt}, waiting ${delay}ms...`, error),
      }
    )
  } catch (error) {
    return { error: error instanceof Error ? error.message : String(error) }
  }
}

// ===== è‡ªåŠ¨ä¿®å¤ =====

async function autoFix(
  toolCalls: any[],
  workspacePath: string,
  assistantId: string | null
): Promise<void> {
  const store = useAgentStore.getState()
  const writeToolCalls = toolCalls.filter(tc => !READ_TOOLS.includes(tc.name))
  if (writeToolCalls.length === 0) return

  const editedFiles = writeToolCalls
    .filter(tc => isFileEditTool(tc.name))
    .map(tc => {
      const path = tc.arguments.path as string
      return pathStartsWith(path, workspacePath) ? path : joinPath(workspacePath, path)
    })
    .filter(path => !path.endsWith('/'))

  if (editedFiles.length === 0) return

  // å¹¶è¡Œæ£€æŸ¥æ‰€æœ‰æ–‡ä»¶çš„ lint é”™è¯¯
  const results = await Promise.all(
    editedFiles.map(async (filePath) => {
      try {
        const result = await toolRegistry.execute('get_lint_errors', { path: filePath }, { workspacePath })
        if (result.success && result.result) {
          const text = result.result.trim()
          if (text && text !== '[]' && text !== 'No diagnostics found') {
            if (/\[error\]/i.test(text) || text.includes('failed to compile') || text.includes('syntax error')) {
              return `File: ${filePath}\n${text}`
            }
          }
        }
      } catch { /* ignore */ }
      return null
    })
  )

  const errors = results.filter((e): e is string => e !== null)

  if (errors.length > 0 && assistantId) {
    store.appendToAssistant(assistantId, `\n\nğŸ” **Auto-check**: Detected ${errors.length} issue(s). Attempting to fix...`)
  }
}

// ===== å‹ç¼©æ£€æŸ¥ä¸å¤„ç† =====

interface CompressionCheckResult {
  level: 0 | 1 | 2 | 3 | 4
  needsHandoff: boolean
}

/**
 * æ£€æŸ¥å¹¶å¤„ç†å‹ç¼©
 * 
 * åœ¨ LLM è¿”å›åè°ƒç”¨ï¼Œæ ¹æ®çœŸå® token ä½¿ç”¨é‡æ›´æ–°å‹ç¼©ç»Ÿè®¡
 */
async function checkAndHandleCompression(
  usage: { input: number; output: number },
  contextLimit: number,
  store: ReturnType<typeof useAgentStore.getState>,
  context: ExecutionContext,
  assistantId: string,
  enableLLMSummary: boolean,
  autoHandoff: boolean
): Promise<CompressionCheckResult> {
  const thread = store.getCurrentThread()
  const messageCount = thread?.messages.length || 0
  
  // ä½¿ç”¨ CompressionManager æ›´æ–°ç»Ÿè®¡
  const previousStats = store.compressionStats
  const newStats = updateStats(
    { promptTokens: usage.input, completionTokens: usage.output },
    contextLimit,
    previousStats,
    messageCount
  )
  
  // ä½¿ç”¨çœŸå® usage è®¡ç®—çš„ç­‰çº§
  // æ³¨æ„ï¼šè¿™é‡Œä¸å†å¼ºåˆ¶"åªå‡ä¸é™"ï¼Œè®© MessageBuilder åœ¨å‘é€å‰åŠ¨æ€è°ƒæ•´
  // ä½†å¦‚æœä¹‹å‰åº”ç”¨äº†æ›´é«˜ç­‰çº§çš„å‹ç¼©ï¼Œä¿ç•™é‚£ä¸ªç­‰çº§ç”¨äº L3/L4 çš„ç‰¹æ®Šå¤„ç†
  const previousLevel = (previousStats?.level ?? 0) as 0 | 1 | 2 | 3 | 4
  const calculatedLevel = newStats.level
  
  // åªæœ‰ L3/L4 éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆç”Ÿæˆæ‘˜è¦/handoffï¼‰ï¼Œå…¶ä»–æƒ…å†µç”¨è®¡ç®—å‡ºçš„ç­‰çº§
  const finalLevel = calculatedLevel >= 3 || previousLevel >= 3
    ? Math.max(previousLevel, calculatedLevel) as 0 | 1 | 2 | 3 | 4
    : calculatedLevel
  
  // æ›´æ–°ä¸ºæœ€ç»ˆç­‰çº§
  newStats.level = finalLevel
  newStats.levelName = LEVEL_NAMES[finalLevel]
  newStats.needsHandoff = finalLevel >= 4
  
  const { ratio, inputTokens, outputTokens } = newStats
  
  logger.agent.info(
    `[Compression] L${finalLevel} (${LEVEL_NAMES[finalLevel]}), ` +
    `ratio: ${(ratio * 100).toFixed(1)}%, ` +
    `tokens: ${inputTokens + outputTokens}/${contextLimit}` +
    (calculatedLevel !== finalLevel ? ` (kept from L${previousLevel})` : '')
  )

  // æ›´æ–° store
  store.setCompressionStats(newStats as any)
  store.setCompressionPhase('idle')

  // L3: ç”Ÿæˆ LLM æ‘˜è¦
  if (finalLevel >= 3 && enableLLMSummary && thread) {
    store.setCompressionPhase('summarizing')
    try {
      const userTurns = thread.messages.filter(m => m.role === 'user').length
      const summaryResult = await generateSummary(thread.messages, { type: 'detailed' })
      store.setContextSummary({
        objective: summaryResult.objective,
        completedSteps: summaryResult.completedSteps,
        pendingSteps: summaryResult.pendingSteps,
        decisions: [],
        fileChanges: summaryResult.fileChanges,
        errorsAndFixes: [],
        userInstructions: [],
        generatedAt: Date.now(),
        turnRange: [0, userTurns],
      })
      EventBus.emit({ type: 'context:summary', summary: summaryResult.summary })
    } catch {
      // æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
    }
    store.setCompressionPhase('idle')
  }

  // L4: ç”Ÿæˆ Handoff æ–‡æ¡£
  if (finalLevel >= 4) {
    if (autoHandoff && thread && context.workspacePath) {
      store.setCompressionPhase('summarizing')
      try {
        const handoff = await generateHandoffDocument(thread.id, thread.messages, context.workspacePath)
        store.setHandoffDocument(handoff)
        EventBus.emit({ type: 'context:handoff', document: handoff })
      } catch {
        // Handoff ç”Ÿæˆå¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
      }
      store.setCompressionPhase('idle')
    }

    const { language } = useStore.getState()
    const msg = language === 'zh'
      ? 'âš ï¸ **ä¸Šä¸‹æ–‡å·²æ»¡**\n\nå½“å‰å¯¹è¯å·²è¾¾åˆ°ä¸Šä¸‹æ–‡é™åˆ¶ã€‚è¯·å¼€å§‹æ–°ä¼šè¯ç»§ç»­ã€‚'
      : 'âš ï¸ **Context Limit Reached**\n\nPlease start a new session to continue.'
    store.appendToAssistant(assistantId, msg)
    store.setHandoffRequired(true)
  }

  EventBus.emit({ type: 'context:level', level: finalLevel, tokens: inputTokens + outputTokens, ratio })

  return { level: finalLevel, needsHandoff: finalLevel >= 4 }
}

// ===== ä¸»å¾ªç¯ =====

export async function runLoop(
  config: LLMConfig,
  llmMessages: OpenAIMessage[],
  context: ExecutionContext,
  assistantId: string
): Promise<void> {
  const store = useAgentStore.getState()
  const mainStore = useStore.getState()
  
  // ä¸€æ¬¡æ€§è·å–æ‰€æœ‰é…ç½®ï¼Œé¿å…é‡å¤è°ƒç”¨ getState()
  const agentConfig = getAgentConfig()
  const maxIterations = mainStore.agentConfig.maxToolLoops || agentConfig.maxToolLoops
  const enableAutoFix = mainStore.agentConfig.enableAutoFix
  const enableLLMSummary = mainStore.agentConfig.enableLLMSummary
  const autoHandoff = mainStore.agentConfig.autoHandoff ?? agentConfig.autoHandoff

  // è·å–æ¨¡å‹ä¸Šä¸‹æ–‡é™åˆ¶ï¼ˆé»˜è®¤ 128kï¼‰
  const contextLimit = config.contextLimit || 128_000

  const loopDetector = new LoopDetector()
  let iteration = 0
  let shouldContinue = true

  EventBus.emit({ type: 'loop:start' })

  while (shouldContinue && iteration < maxIterations && !context.abortSignal?.aborted) {
    iteration++
    shouldContinue = false
    EventBus.emit({ type: 'loop:iteration', count: iteration })

    if (llmMessages.length === 0) {
      logger.agent.error('[Loop] No messages to send')
      store.appendToAssistant(assistantId, '\n\nâŒ Error: No messages to send')
      EventBus.emit({ type: 'loop:end', reason: 'no_messages' })
      break
    }

    // è°ƒç”¨ LLM
    const result = await callLLMWithRetry(config, llmMessages, context.chatMode, assistantId, context.abortSignal)

    if (context.abortSignal?.aborted) {
      EventBus.emit({ type: 'loop:end', reason: 'aborted' })
      break
    }

    if (result.error) {
      logger.agent.error('[Loop] LLM error:', result.error)
      store.appendToAssistant(assistantId, `\n\nâŒ Error: ${result.error}`)
      EventBus.emit({ type: 'loop:end', reason: 'error' })
      break
    }

    // åœ¨ LLM è°ƒç”¨åç«‹å³æ£€æŸ¥å‹ç¼©ï¼ˆå‚è€ƒ OpenCode çš„ finish-step é€»è¾‘ï¼‰
    if (result.usage) {
      const usage = {
        input: result.usage.promptTokens || 0,
        output: result.usage.completionTokens || 0,
      }

      const compressionResult = await checkAndHandleCompression(
        usage,
        contextLimit,
        store,
        context,
        assistantId,
        enableLLMSummary,
        autoHandoff
      )

      // L4 éœ€è¦ä¸­æ–­å¾ªç¯
      if (compressionResult.needsHandoff) {
        EventBus.emit({ type: 'loop:end', reason: 'handoff_required' })
        break
      }
    }

    // æ²¡æœ‰å·¥å…·è°ƒç”¨
    if (!result.toolCalls || result.toolCalls.length === 0) {
      // Plan æ¨¡å¼æé†’
      if (context.chatMode === 'plan' && store.plan) {
        const readOnlyTools = getReadOnlyTools()
        const hasWriteOps = llmMessages.some(m => m.role === 'assistant' && m.tool_calls?.some((tc: any) => !readOnlyTools.includes(tc.function.name)))
        const hasUpdatePlan = llmMessages.some(m => m.role === 'assistant' && m.tool_calls?.some((tc: any) => tc.function.name === 'update_plan'))
        if (hasWriteOps && !hasUpdatePlan && iteration < maxIterations) {
          llmMessages.push({ role: 'user', content: 'Reminder: Please use `update_plan` to update the plan status before finishing.' })
          shouldContinue = true
          continue
        }
      }
      EventBus.emit({ type: 'loop:end', reason: 'complete' })
      break
    }

    // å¾ªç¯æ£€æµ‹
    const loopCheck = loopDetector.checkLoop(result.toolCalls)
    if (loopCheck.isLoop) {
      logger.agent.warn(`[Loop] Loop detected: ${loopCheck.reason}`)
      const suggestion = loopCheck.suggestion ? `\nğŸ’¡ ${loopCheck.suggestion}` : ''
      store.appendToAssistant(assistantId, `\n\nâš ï¸ ${loopCheck.reason}${suggestion}`)
      EventBus.emit({ type: 'loop:warning', message: loopCheck.reason || 'Loop detected' })
      EventBus.emit({ type: 'loop:end', reason: 'loop_detected' })
      break
    }

    // æ·»åŠ å·¥å…·è°ƒç”¨åˆ° UI
    const currentMsg = store.getMessages().find(m => m.id === assistantId)
    if (currentMsg?.role === 'assistant') {
      const existing = (currentMsg as any).toolCalls || []
      for (const tc of result.toolCalls) {
        if (!existing.find((e: any) => e.id === tc.id)) {
          store.addToolCallPart(assistantId, { id: tc.id, name: tc.name, arguments: tc.arguments })
        }
      }
    }

    // æ·»åŠ åˆ°æ¶ˆæ¯å†å²
    llmMessages.push({
      role: 'assistant',
      content: result.content || null,
      tool_calls: result.toolCalls.map(tc => ({
        id: tc.id,
        type: 'function' as const,
        function: { name: tc.name, arguments: JSON.stringify(tc.arguments) },
      })),
    })

    // æ‰§è¡Œå·¥å…·
    const { results: toolResults, userRejected } = await executeTools(
      result.toolCalls,
      { workspacePath: context.workspacePath, currentAssistantId: assistantId },
      context.abortSignal
    )

    // æ£€æŸ¥ ask_user
    const waitingResult = toolResults.find(r => r.result.meta?.waitingForUser)
    if (waitingResult) {
      // ä» meta ä¸­æå– interactive æ•°æ®å¹¶è®¾ç½®åˆ° store
      const interactive = waitingResult.result.meta?.interactive as import('../types').InteractiveContent | undefined
      if (interactive) {
        store.setInteractive(assistantId, interactive)
      } else {
        // å…œåº•ï¼šå¦‚æœæ²¡æœ‰ interactive æ•°æ®ï¼Œè‡³å°‘è¦ finalize
        store.finalizeAssistant(assistantId)
      }
      store.setStreamPhase('idle')
      EventBus.emit({ type: 'loop:end', reason: 'waiting_for_user' })
      break
    }

    // æ·»åŠ å·¥å…·ç»“æœ
    for (const { toolCall, result: toolResult } of toolResults) {
      llmMessages.push({
        role: 'tool' as const,
        tool_call_id: toolCall.id,
        name: toolCall.name,
        content: toolResult.content,
      })
      const meta = toolResult.meta
      if (meta?.filePath && typeof meta.filePath === 'string' && typeof meta.newContent === 'string') {
        loopDetector.updateContentHash(meta.filePath, meta.newContent)
        
        // æ·»åŠ å¾…ç¡®è®¤çš„æ–‡ä»¶å˜æ›´
        store.addPendingChange({
          filePath: meta.filePath,
          toolCallId: toolCall.id,
          toolName: toolCall.name,
          changeType: meta.oldContent ? 'modify' : 'create',
          snapshot: {
            path: meta.filePath,
            content: (meta.oldContent as string) || null,
            timestamp: Date.now(),
          },
          newContent: meta.newContent,
          linesAdded: (meta.linesAdded as number) || 0,
          linesRemoved: (meta.linesRemoved as number) || 0,
        })
      }
    }

    // è‡ªåŠ¨ä¿®å¤ï¼ˆå¹¶è¡Œæ£€æŸ¥ï¼‰
    if (enableAutoFix && !userRejected && context.workspacePath) {
      await autoFix(result.toolCalls, context.workspacePath, assistantId)
    }

    if (userRejected) {
      EventBus.emit({ type: 'loop:end', reason: 'user_rejected' })
      break
    }

    shouldContinue = true
    store.setStreamPhase('streaming')
  }

  if (iteration >= maxIterations) {
    store.appendToAssistant(assistantId, '\n\nâš ï¸ Reached maximum tool call limit.')
    EventBus.emit({ type: 'loop:warning', message: 'Max iterations reached' })
    EventBus.emit({ type: 'loop:end', reason: 'max_iterations' })
  }

  // å¾ªç¯ç»“æŸï¼Œfinalize ç”± Agent.ts çš„ cleanup å¤„ç†
}
