/**
 * Agent ä¸»å¾ªç¯
 * 
 * èŒè´£ï¼š
 * - ç®¡ç† LLM è°ƒç”¨å¾ªç¯
 * - åŸºäºçœŸå® token ä½¿ç”¨é‡çš„ä¸Šä¸‹æ–‡å‹ç¼©
 * - å·¥å…·æ‰§è¡Œåè°ƒ
 * - å¾ªç¯æ£€æµ‹
 * - å‘å¸ƒäº‹ä»¶åˆ° EventBus
 */

import { api } from '@/renderer/services/electronAPI'
import { logger } from '@utils/Logger'
import { performanceMonitor, withRetry, isRetryableError } from '@shared/utils'
import { useAgentStore } from '../store/AgentStore'
import { useStore } from '@store'
import { toolManager, initializeToolProviders, setToolLoadingContext, initializeTools } from '../tools'
import { toolRegistry } from '../tools/registry'
import { getAgentConfig, READ_TOOLS } from '../utils/AgentConfig'
import { LoopDetector } from '../utils/LoopDetector'
import { getReadOnlyTools, isFileEditTool } from '@/shared/config/tools'
import { pathStartsWith, joinPath } from '@shared/utils/pathUtils'
import { createStreamProcessor } from './stream'
import { executeTools } from './tools'
import { EventBus } from './EventBus'
import {
  generateSummary,
  generateHandoffDocument,
} from '../context'
import { updateStats, LEVEL_NAMES, estimateMessagesTokens } from '../context/CompressionManager'
import type { ChatMessage } from '../types'
import type { LLMMessage } from '@/shared/types'
import type { WorkMode } from '@/renderer/modes/types'
import type { LLMConfig, LLMCallResult, ExecutionContext } from './types'

// ===== æ¨¡å¼åå¤„ç†é’©å­ =====

/**
 * æ‰§è¡Œæ¨¡å¼åå¤„ç†é’©å­
 */
function executeModePostProcessHook(
  mode: WorkMode,
  context: Parameters<import('@shared/config/agentConfig').ModePostProcessHook>[0]
): ReturnType<import('@shared/config/agentConfig').ModePostProcessHook> {
  const agentConfig = getAgentConfig()
  const hookConfig = agentConfig.modePostProcessHooks?.[mode]

  if (!hookConfig?.enabled || !hookConfig.hook) {
    return null
  }

  try {
    return hookConfig.hook(context)
  } catch (error) {
    logger.agent.error(`[Loop] Mode post-process hook error for ${mode}:`, error)
    return null
  }
}

// ===== LLM è°ƒç”¨ =====

/**
 * è°ƒç”¨ LLM å¹¶å¤„ç†æµå¼å“åº”
 * 
 * @param config - LLM é…ç½®
 * @param messages - æ¶ˆæ¯å†å²
 * @param chatMode - å·¥ä½œæ¨¡å¼
 * @param assistantId - åŠ©æ‰‹æ¶ˆæ¯ ID
 * @param threadStore - çº¿ç¨‹ç»‘å®šçš„ Store
 * @returns LLM è°ƒç”¨ç»“æœ
 */
async function callLLM(
  config: LLMConfig,
  messages: LLMMessage[],
  chatMode: WorkMode,
  assistantId: string | null,
  threadStore: import('../store/AgentStore').ThreadBoundStore
): Promise<LLMCallResult> {
  performanceMonitor.start(`llm:${config.model}`, 'llm', { provider: config.provider, messageCount: messages.length })

  const processor = createStreamProcessor(assistantId, threadStore)

  try {
    // åˆå§‹åŒ–å·¥å…·
    initializeToolProviders()
    await initializeTools()
    const templateId = useStore.getState().promptTemplateId
    setToolLoadingContext({
      mode: chatMode,
      templateId,
    })
    const tools = chatMode === 'chat' ? [] : toolManager.getAllToolDefinitions()

    // åŠ¨æ€å·¥å…·æ§åˆ¶ï¼šæ ¹æ®ä¸Šä¸‹æ–‡é™åˆ¶å¯ç”¨å·¥å…·
    let activeTools: string[] | undefined

    if (tools.length > 0) {
      const allToolNames = tools.map(t => t.name)
      const store = useAgentStore.getState()

      // åœºæ™¯1: Chat æ¨¡å¼ - ç¦ç”¨æ‰€æœ‰å·¥å…·ï¼ˆå·²åœ¨ä¸Šé¢å¤„ç†ï¼‰
      // åœºæ™¯2: Plan æ¨¡å¼ - å¯ç”¨æ‰€æœ‰å·¥å…·ï¼ˆåŒ…æ‹¬ plan ç›¸å…³å·¥å…·ï¼‰
      // åœºæ™¯3: Code æ¨¡å¼ - æ ¹æ®å‹ç¼©ç­‰çº§åŠ¨æ€è°ƒæ•´

      // å½“ä¸Šä¸‹æ–‡å‹ç¼©ç­‰çº§è¾ƒé«˜æ—¶ï¼Œé™åˆ¶å·¥å…·ä»¥å‡å°‘ token ä½¿ç”¨
      const currentThread = store.getCurrentThread()
      const compressionLevel = currentThread?.compressionStats?.level || 0
      if (compressionLevel >= 3) {
        // L3/L4: åªä¿ç•™æ ¸å¿ƒå·¥å…·ï¼Œç§»é™¤ AI è¾…åŠ©å·¥å…·ï¼ˆèŠ‚çœ tokenï¼‰
        const coreTools = allToolNames.filter(name =>
          !['analyze_code', 'suggest_refactoring', 'suggest_fixes', 'generate_tests'].includes(name)
        )
        activeTools = coreTools
        logger.agent.info(`[Loop] Compression L${compressionLevel}: ${activeTools.length}/${allToolNames.length} tools active (AI tools disabled)`)
      }

      // æœªæ¥å¯æ‰©å±•çš„åœºæ™¯ï¼š
      // - åªè¯»æ¨¡å¼ï¼šactiveTools = allToolNames.filter(name => getReadOnlyTools().includes(name))
      // - å®‰å…¨æ¨¡å¼ï¼šactiveTools = allToolNames.filter(name => !getDangerousTools().includes(name))
      // - ç‰¹å®šä»»åŠ¡ï¼šactiveTools = getToolsForTask(taskType)
    }

    // å‘é€è¯·æ±‚
    await api.llm.send({
      config: config as import('@shared/types/llm').LLMConfig,
      messages: messages as LLMMessage[],
      tools,
      systemPrompt: '',
      activeTools
    })

    // ç­‰å¾…æµå¼å“åº”å®Œæˆ
    const result = await processor.wait()
    performanceMonitor.end(`llm:${config.model}`, !result.error)

    // æ›´æ–° usage
    if (assistantId && result.usage) {
      useAgentStore.getState().updateMessage(assistantId, {
        usage: result.usage
      } as Partial<import('../types').AssistantMessage>)
    } else if (assistantId && !result.usage) {
      logger.agent.warn('[Loop] No usage data in LLM result')
    }

    processor.cleanup()
    return result
  } catch (error) {
    processor.cleanup()
    logger.agent.error('[Loop] Error in callLLM:', error)

    const errorMsg = error instanceof Error ? error.message : String(error)
    return { error: errorMsg }
  }
}

async function callLLMWithRetry(
  config: LLMConfig,
  messages: LLMMessage[],
  chatMode: WorkMode,
  assistantId: string | null,
  threadStore: import('../store/AgentStore').ThreadBoundStore,
  abortSignal?: AbortSignal
): Promise<LLMCallResult> {
  const retryConfig = getAgentConfig()
  try {
    return await withRetry(
      async () => {
        if (abortSignal?.aborted) throw new Error('Aborted')
        const result = await callLLM(config, messages, chatMode, assistantId, threadStore)

        // å·¥å…·è°ƒç”¨è§£æé”™è¯¯ä¸åº”è¯¥å¯¼è‡´é‡è¯•ï¼Œè€Œæ˜¯è¿”å›ç»™ AI è®©å®ƒåæ€
        // åªæœ‰çœŸæ­£çš„ LLM é”™è¯¯ï¼ˆç½‘ç»œã€API ç­‰ï¼‰æ‰éœ€è¦é‡è¯•
        if (result.error) {
          const errorMsg = result.error.toLowerCase()
          const isToolParseError = errorMsg.includes('tool call parse') ||
            errorMsg.includes('invalid input for tool') ||
            errorMsg.includes('type validation failed')

          if (isToolParseError) {
            // å·¥å…·è§£æé”™è¯¯ï¼šä¸é‡è¯•ï¼Œè¿”å›ç»“æœè®© loop å¤„ç†
            logger.agent.warn('[Loop] Tool parse error, will be handled in loop:', result.error)
            return result
          }

          // å…¶ä»–é”™è¯¯ï¼šæŠ›å‡ºä»¥è§¦å‘é‡è¯•
          throw new Error(result.error)
        }

        return result
      },
      {
        maxRetries: retryConfig.maxRetries,
        initialDelayMs: retryConfig.retryDelayMs,
        backoffMultiplier: retryConfig.retryBackoffMultiplier,
        isRetryable: error => {
          const msg = error instanceof Error ? error.message : String(error)
          return isRetryableError(error) && msg !== 'Aborted'
        },
        onRetry: (attempt, error, delay) =>
          logger.agent.info(`[Loop] LLM retry ${attempt}, waiting ${delay}ms...`, error),
      }
    )
  } catch (error) {
    return { error: error instanceof Error ? error.message : String(error) }
  }
}

// ===== è‡ªåŠ¨ä¿®å¤ =====

async function autoFix(
  toolCalls: any[],
  workspacePath: string,
  assistantId: string | null
): Promise<void> {
  const store = useAgentStore.getState()
  const writeToolCalls = toolCalls.filter(tc => !READ_TOOLS.includes(tc.name))
  if (writeToolCalls.length === 0) return

  const editedFiles = writeToolCalls
    .filter(tc => isFileEditTool(tc.name))
    .map(tc => {
      const path = tc.arguments.path as string
      return pathStartsWith(path, workspacePath) ? path : joinPath(workspacePath, path)
    })
    .filter(path => !path.endsWith('/'))

  if (editedFiles.length === 0) return

  // å¹¶è¡Œæ£€æŸ¥æ‰€æœ‰æ–‡ä»¶çš„ lint é”™è¯¯
  const results = await Promise.all(
    editedFiles.map(async (filePath) => {
      try {
        const result = await toolRegistry.execute('get_lint_errors', { path: filePath }, { workspacePath })
        if (result.success && result.result) {
          const text = result.result.trim()
          if (text && text !== '[]' && text !== 'No diagnostics found') {
            if (/\[error\]/i.test(text) || text.includes('failed to compile') || text.includes('syntax error')) {
              return `File: ${filePath}\n${text}`
            }
          }
        }
      } catch { /* ignore */ }
      return null
    })
  )

  const errors = results.filter((e): e is string => e !== null)

  if (errors.length > 0 && assistantId) {
    store.appendToAssistant(assistantId, `\n\nğŸ” **Auto-check**: Detected ${errors.length} issue(s). Attempting to fix...`)
  }
}

// ===== å‹ç¼©æ£€æŸ¥ä¸å¤„ç† =====

interface CompressionCheckResult {
  level: 0 | 1 | 2 | 3 | 4
  needsHandoff: boolean
}

/**
 * æ£€æŸ¥å¹¶å¤„ç†å‹ç¼©
 * 
 * åœ¨ LLM è¿”å›åè°ƒç”¨ï¼Œæ ¹æ®çœŸå® token ä½¿ç”¨é‡æ›´æ–°å‹ç¼©ç»Ÿè®¡
 */
async function checkAndHandleCompression(
  usage: { input: number; output: number },
  contextLimit: number,
  store: ReturnType<typeof useAgentStore.getState>,
  threadStore: import('../store/AgentStore').ThreadBoundStore,
  context: ExecutionContext,
  assistantId: string,
  enableLLMSummary: boolean,
  autoHandoff: boolean
): Promise<CompressionCheckResult> {
  const thread = store.getCurrentThread()
  const messageCount = thread?.messages.length || 0

  // ä½¿ç”¨ CompressionManager æ›´æ–°ç»Ÿè®¡ï¼ˆä½¿ç”¨çœŸå® usageï¼‰
  const previousStats = thread?.compressionStats || null
  const newStats = updateStats(
    { promptTokens: usage.input, completionTokens: usage.output },
    contextLimit,
    previousStats,
    messageCount
  )

  // ä½¿ç”¨çœŸå® usage è®¡ç®—çš„ç­‰çº§ï¼ˆä¸å†å¼ºåˆ¶"åªå‡ä¸é™"ï¼‰
  const calculatedLevel = newStats.level

  logger.agent.info(
    `[Compression] L${calculatedLevel} (${LEVEL_NAMES[calculatedLevel]}), ` +
    `ratio: ${(newStats.ratio * 100).toFixed(1)}%, ` +
    `tokens: ${newStats.inputTokens + newStats.outputTokens}/${contextLimit}`
  )

  // æ›´æ–° storeï¼ˆä½¿ç”¨ threadStore ç¡®ä¿çº¿ç¨‹éš”ç¦»ï¼‰
  threadStore.setCompressionStats(newStats as import('../context/CompressionManager').CompressionStats)
  threadStore.setCompressionPhase('idle')

  // L3 é¢„è­¦ï¼šæå‰é€šçŸ¥ç”¨æˆ·ä¸Šä¸‹æ–‡å³å°†æ»¡
  if (calculatedLevel === 3 && (!previousStats || previousStats.level < 3)) {
    const remainingRatio = 1 - newStats.ratio
    const estimatedRemainingTurns = Math.floor(remainingRatio * contextLimit / (usage.input + usage.output))
    EventBus.emit({
      type: 'context:warning',
      level: 3,
      message: `Context usage is high (${(newStats.ratio * 100).toFixed(1)}%). Estimated ${estimatedRemainingTurns} turns remaining.`,
    })
  }

  // L3: ç”Ÿæˆ LLM æ‘˜è¦
  if (calculatedLevel >= 3 && enableLLMSummary && thread) {
    threadStore.setCompressionPhase('summarizing')
    try {
      const userTurns = thread.messages.filter(m => m.role === 'user').length
      const summaryResult = await generateSummary(thread.messages, { type: 'detailed' })
      threadStore.setContextSummary({
        objective: summaryResult.objective,
        completedSteps: summaryResult.completedSteps,
        pendingSteps: summaryResult.pendingSteps,
        decisions: [],
        fileChanges: summaryResult.fileChanges,
        errorsAndFixes: [],
        userInstructions: [],
        generatedAt: Date.now(),
        turnRange: [0, userTurns],
      })
      EventBus.emit({ type: 'context:summary', summary: summaryResult.summary })
    } catch {
      // æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
    }
    threadStore.setCompressionPhase('idle')
  }

  // L4: ç”Ÿæˆ Handoff æ–‡æ¡£
  if (calculatedLevel >= 4) {
    if (autoHandoff && thread && context.workspacePath) {
      threadStore.setCompressionPhase('summarizing')
      try {
        const handoff = await generateHandoffDocument(thread.id, thread.messages, context.workspacePath)
        store.setHandoffDocument(handoff)  // handoffDocument æ˜¯å…¨å±€çŠ¶æ€ï¼Œä¿æŒä½¿ç”¨ store
        EventBus.emit({ type: 'context:handoff', document: handoff })
      } catch {
        // Handoff ç”Ÿæˆå¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
      }
      threadStore.setCompressionPhase('idle')
    }

    const { language } = useStore.getState()
    const msg = language === 'zh'
      ? 'âš ï¸ **ä¸Šä¸‹æ–‡å·²æ»¡**\n\nå½“å‰å¯¹è¯å·²è¾¾åˆ°ä¸Šä¸‹æ–‡é™åˆ¶ã€‚è¯·å¼€å§‹æ–°ä¼šè¯ç»§ç»­ã€‚'
      : 'âš ï¸ **Context Limit Reached**\n\nPlease start a new session to continue.'
    threadStore.appendToAssistant(assistantId, msg)
    threadStore.setHandoffRequired(true)
  }

  EventBus.emit({ type: 'context:level', level: calculatedLevel, tokens: newStats.inputTokens + newStats.outputTokens, ratio: newStats.ratio })

  return { level: calculatedLevel, needsHandoff: calculatedLevel >= 4 }
}

// ===== ä¸»å¾ªç¯ =====

export async function runLoop(
  config: LLMConfig,
  llmMessages: LLMMessage[],
  context: ExecutionContext,
  assistantId: string
): Promise<void> {
  const store = useAgentStore.getState()
  const mainStore = useStore.getState()

  // åˆ›å»ºçº¿ç¨‹ç»‘å®šçš„ Storeï¼ˆç¡®ä¿åå°ä»»åŠ¡ä¸ä¼šå½±å“å…¶ä»–çº¿ç¨‹ï¼‰
  const threadId = context.threadId || store.currentThreadId
  if (!threadId) {
    logger.agent.error('[Loop] No thread ID available')
    return
  }
  const threadStore = store.forThread(threadId)

  // ä¸€æ¬¡æ€§è·å–æ‰€æœ‰é…ç½®ï¼Œé¿å…é‡å¤è°ƒç”¨ getState()
  const agentConfig = getAgentConfig()
  const maxIterations = mainStore.agentConfig.maxToolLoops || agentConfig.maxToolLoops
  const enableAutoFix = mainStore.agentConfig.enableAutoFix
  const enableLLMSummary = mainStore.agentConfig.enableLLMSummary
  const autoHandoff = mainStore.agentConfig.autoHandoff ?? agentConfig.autoHandoff

  // è·å–æ¨¡å‹ä¸Šä¸‹æ–‡é™åˆ¶ï¼ˆé»˜è®¤ 128kï¼‰
  const contextLimit = config.contextLimit || 128_000

  const loopDetector = new LoopDetector()
  let iteration = 0
  let shouldContinue = true

  EventBus.emit({ type: 'loop:start' })

  while (shouldContinue && iteration < maxIterations && !context.abortSignal?.aborted) {
    iteration++
    shouldContinue = false
    EventBus.emit({ type: 'loop:iteration', count: iteration })

    // æ£€æŸ¥ä¸­æ­¢ä¿¡å·
    if (context.abortSignal?.aborted) {
      EventBus.emit({ type: 'loop:end', reason: 'aborted' })
      break
    }

    if (llmMessages.length === 0) {
      logger.agent.error('[Loop] No messages to send')
      threadStore.appendToAssistant(assistantId, '\n\nâŒ Error: No messages to send')
      EventBus.emit({ type: 'loop:end', reason: 'no_messages' })
      break
    }

    // è°ƒç”¨ LLM
    const result = await callLLMWithRetry(config, llmMessages, context.chatMode, assistantId, threadStore, context.abortSignal)

    // å†æ¬¡æ£€æŸ¥ä¸­æ­¢ä¿¡å·ï¼ˆLLM è°ƒç”¨åï¼‰
    if (context.abortSignal?.aborted) {
      EventBus.emit({ type: 'loop:end', reason: 'aborted' })
      break
    }

    // å¤„ç†é”™è¯¯
    if (result.error) {
      const errorMsg = result.error.toLowerCase()
      const isToolParseError = errorMsg.includes('tool call parse') ||
        errorMsg.includes('invalid input for tool') ||
        errorMsg.includes('type validation failed')

      if (isToolParseError) {
        // å·¥å…·è§£æé”™è¯¯ï¼šä½œä¸ºç”¨æˆ·æ¶ˆæ¯è¿”å›ç»™ AIï¼Œè®©å®ƒåæ€å’Œé‡è¯•
        logger.agent.warn('[Loop] Tool parse error, adding as feedback:', result.error)

        llmMessages.push({
          role: 'user',
          content: `âŒ Tool Call Error: ${result.error}

Please fix the tool call and try again. Make sure:
1. All required parameters are provided
2. Parameter types are correct
3. Parameter names match exactly

Try again with the corrected tool call.`
        })

        shouldContinue = true
        continue
      } else {
        // å…¶ä»–é”™è¯¯ï¼šä¸­æ­¢å¾ªç¯
        logger.agent.error('[Loop] LLM error:', result.error)
        threadStore.appendToAssistant(assistantId, `\n\nâŒ Error: ${result.error}`)
        EventBus.emit({ type: 'loop:end', reason: 'error' })
        break
      }
    }

    // åœ¨ LLM è°ƒç”¨åç«‹å³æ£€æŸ¥å‹ç¼©
    // å¤„ç† usage å¯èƒ½æ˜¯æ•°ç»„æˆ–å¯¹è±¡çš„æƒ…å†µ
    const usageData = Array.isArray(result.usage) ? result.usage[0] : result.usage

    if (usageData && usageData.totalTokens > 0) {
      const usage = {
        input: usageData.promptTokens || 0,
        output: usageData.completionTokens || 0,
      }

      const compressionResult = await checkAndHandleCompression(
        usage,
        contextLimit,
        store,
        threadStore,
        context,
        assistantId,
        enableLLMSummary,
        autoHandoff
      )

      // L4 éœ€è¦ä¸­æ–­å¾ªç¯
      if (compressionResult.needsHandoff) {
        EventBus.emit({ type: 'loop:end', reason: 'handoff_required' })
        break
      }
    } else {
      // å…œåº•ï¼šä½¿ç”¨ç²¾ç¡®ä¼°ç®—å€¼æ›´æ–°ç»Ÿè®¡
      logger.agent.warn('[Loop] No valid usage data from LLM, using estimated tokens')

      const estimatedTokens = estimateMessagesTokens(llmMessages as ChatMessage[])

      // å‡è®¾ 90% æ˜¯è¾“å…¥ï¼Œ10% æ˜¯è¾“å‡ºï¼ˆä¿å®ˆä¼°è®¡ï¼‰
      const usage = {
        input: Math.floor(estimatedTokens * 0.9),
        output: Math.floor(estimatedTokens * 0.1),
      }

      // æ›´æ–°æ¶ˆæ¯çš„ usageï¼ˆä½¿ç”¨ä¼°ç®—å€¼ï¼‰
      if (assistantId) {
        store.updateMessage(assistantId, {
          usage: {
            promptTokens: usage.input,
            completionTokens: usage.output,
            totalTokens: usage.input + usage.output,
          }
        } as Partial<import('../types').AssistantMessage>)
      }

      const compressionResult = await checkAndHandleCompression(
        usage,
        contextLimit,
        store,
        threadStore,
        context,
        assistantId,
        enableLLMSummary,
        autoHandoff
      )

      // L4 éœ€è¦ä¸­æ–­å¾ªç¯
      if (compressionResult.needsHandoff) {
        EventBus.emit({ type: 'loop:end', reason: 'handoff_required' })
        break
      }
    }

    // æ²¡æœ‰å·¥å…·è°ƒç”¨ - Chat æ¨¡å¼æˆ– LLM å†³å®šç»“æŸ
    if (!result.toolCalls || result.toolCalls.length === 0) {
      // æ¨¡å¼åå¤„ç†é’©å­
      const hookResult = executeModePostProcessHook(context.chatMode, {
        mode: context.chatMode,
        messages: llmMessages,
        hasWriteOps: llmMessages.some(m => {
          const readOnlyTools = getReadOnlyTools()
          return m.role === 'assistant' && m.tool_calls?.some((tc: any) => !readOnlyTools.includes(tc.function.name))
        }),
        hasSpecificTool: (toolName: string) => llmMessages.some(m =>
          m.role === 'assistant' && m.tool_calls?.some((tc: any) => tc.function.name === toolName)
        ),
        iteration,
        maxIterations,
      })

      if (hookResult?.shouldContinue && hookResult.reminderMessage) {
        llmMessages.push({ role: 'user', content: hookResult.reminderMessage })
        shouldContinue = true
        continue
      }
      EventBus.emit({ type: 'loop:end', reason: 'complete' })
      break
    }

    // å¾ªç¯æ£€æµ‹
    const loopCheck = loopDetector.checkLoop(result.toolCalls)
    if (loopCheck.isLoop) {
      logger.agent.warn(`[Loop] Loop detected: ${loopCheck.reason}`)
      const suggestion = loopCheck.suggestion ? `\nğŸ’¡ ${loopCheck.suggestion}` : ''
      threadStore.appendToAssistant(assistantId, `\n\nâš ï¸ ${loopCheck.reason}${suggestion}`)
      EventBus.emit({ type: 'loop:warning', message: loopCheck.reason || 'Loop detected' })
      EventBus.emit({ type: 'loop:end', reason: 'loop_detected' })
      break
    }

    // æ·»åŠ å·¥å…·è°ƒç”¨åˆ° UI
    const currentMsg = store.getMessages().find(m => m.id === assistantId)
    if (currentMsg?.role === 'assistant') {
      const assistantMsg = currentMsg as import('../types').AssistantMessage
      const existing = assistantMsg.toolCalls || []
      for (const tc of result.toolCalls) {
        if (!existing.find((e) => e.id === tc.id)) {
          threadStore.addToolCallPart(assistantId, { id: tc.id, name: tc.name, arguments: tc.arguments })
        }
      }
    }

    // æ·»åŠ åˆ°æ¶ˆæ¯å†å²
    llmMessages.push({
      role: 'assistant',
      content: result.content || null,
      tool_calls: result.toolCalls.map(tc => ({
        id: tc.id,
        type: 'function' as const,
        function: { name: tc.name, arguments: JSON.stringify(tc.arguments) },
      })),
    })

    // æ‰§è¡Œå·¥å…·
    const { results: toolResults, userRejected } = await executeTools(
      result.toolCalls,
      { workspacePath: context.workspacePath, currentAssistantId: assistantId, chatMode: context.chatMode },
      threadStore,
      context.abortSignal
    )

    // æ£€æŸ¥ä¸­æ­¢ä¿¡å·ï¼ˆå·¥å…·æ‰§è¡Œåï¼‰
    if (context.abortSignal?.aborted) {
      EventBus.emit({ type: 'loop:end', reason: 'aborted' })
      break
    }

    // æ£€æŸ¥ ask_user
    const waitingResult = toolResults.find(r => r.result.meta?.waitingForUser)
    if (waitingResult) {
      // ä» meta ä¸­æå– interactive æ•°æ®å¹¶è®¾ç½®åˆ° store
      const interactive = waitingResult.result.meta?.interactive as import('../types').InteractiveContent | undefined
      if (interactive) {
        threadStore.setInteractive(assistantId, interactive)
      } else {
        // å…œåº•ï¼šå¦‚æœæ²¡æœ‰ interactive æ•°æ®ï¼Œè‡³å°‘è¦ finalize
        threadStore.finalizeAssistant(assistantId)
      }
      threadStore.setStreamPhase('idle')
      EventBus.emit({ type: 'loop:end', reason: 'waiting_for_user' })
      break
    }

    // æ·»åŠ å·¥å…·ç»“æœ
    for (const { toolCall, result: toolResult } of toolResults) {
      llmMessages.push({
        role: 'tool' as const,
        tool_call_id: toolCall.id,
        name: toolCall.name,
        content: toolResult.content,
      })

      // è®°å½•å·¥å…·æ‰§è¡Œç»“æœåˆ°å¾ªç¯æ£€æµ‹å™¨
      const success = !toolResult.content.startsWith('Error:')
      loopDetector.recordResult(toolCall.id, success)

      const meta = toolResult.meta
      if (meta?.filePath && typeof meta.filePath === 'string' && typeof meta.newContent === 'string') {
        loopDetector.updateContentHash(meta.filePath, meta.newContent)

        // æ·»åŠ å¾…ç¡®è®¤çš„æ–‡ä»¶å˜æ›´
        store.addPendingChange({
          filePath: meta.filePath,
          toolCallId: toolCall.id,
          toolName: toolCall.name,
          changeType: meta.oldContent ? 'modify' : 'create',
          snapshot: {
            path: meta.filePath,
            content: (meta.oldContent as string) || null,
            timestamp: Date.now(),
          },
          newContent: meta.newContent,
          linesAdded: (meta.linesAdded as number) || 0,
          linesRemoved: (meta.linesRemoved as number) || 0,
        })
      }
    }

    // è‡ªåŠ¨ä¿®å¤ï¼ˆå¹¶è¡Œæ£€æŸ¥ï¼‰
    if (enableAutoFix && !userRejected && context.workspacePath) {
      await autoFix(result.toolCalls, context.workspacePath, assistantId)
    }

    if (userRejected) {
      EventBus.emit({ type: 'loop:end', reason: 'user_rejected' })
      break
    }

    shouldContinue = true
    threadStore.setStreamPhase('streaming')
  }

  // è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
  if (iteration >= maxIterations) {
    logger.agent.warn('[Loop] Reached maximum iterations')
    threadStore.appendToAssistant(assistantId, '\n\nâš ï¸ Reached maximum tool call limit.')
    EventBus.emit({ type: 'loop:warning', message: 'Max iterations reached' })
    EventBus.emit({ type: 'loop:end', reason: 'max_iterations' })
  }
}
