/**
 * Agent ä¸»å¾ªç¯
 * 
 * èŒè´£ï¼š
 * - ç®¡ç† LLM è°ƒç”¨å¾ªç¯
 * - åŸºäºçœŸå® token ä½¿ç”¨é‡çš„ä¸Šä¸‹æ–‡å‹ç¼©
 * - å·¥å…·æ‰§è¡Œåè°ƒ
 * - å¾ªç¯æ£€æµ‹
 * - å‘å¸ƒäº‹ä»¶åˆ° EventBus
 */

import { api } from '@/renderer/services/electronAPI'
import { logger } from '@utils/Logger'
import { performanceMonitor, withRetry, isRetryableError } from '@shared/utils'
import { useAgentStore } from '../store/AgentStore'
import { useStore } from '@store'
import { toolManager, initializeToolProviders, setToolLoadingContext, initializeTools } from '../tools'
import { toolRegistry } from '../tools/registry'
import { getAgentConfig, READ_TOOLS } from '../utils/AgentConfig'
import { LoopDetector } from '../utils/LoopDetector'
import { getReadOnlyTools, isFileEditTool } from '@/shared/config/tools'
import { pathStartsWith, joinPath } from '@shared/utils/pathUtils'
import { createStreamProcessor } from './stream'
import { executeTools } from './tools'
import { EventBus } from './EventBus'
import { 
  pruneMessages, 
  getCompressionLevel, 
  COMPRESSION_LEVEL_NAMES,
  estimateTokens,
  generateSummary,
  generateHandoffDocument,
} from '../context'
import type { OpenAIMessage } from '../llm/MessageConverter'
import type { WorkMode } from '@/renderer/modes/types'
import type { LLMConfig, LLMCallResult, ExecutionContext } from './types'

// ===== LLM è°ƒç”¨ =====

async function callLLM(
  config: LLMConfig,
  messages: OpenAIMessage[],
  chatMode: WorkMode,
  assistantId: string | null
): Promise<LLMCallResult> {
  performanceMonitor.start(`llm:${config.model}`, 'llm', { provider: config.provider, messageCount: messages.length })

  const processor = createStreamProcessor(assistantId)

  // åˆå§‹åŒ–å·¥å…·
  initializeToolProviders()
  await initializeTools()
  const templateId = useStore.getState().promptTemplateId
  setToolLoadingContext({
    mode: chatMode === 'plan' ? 'plan' : chatMode === 'chat' ? 'chat' : 'code',
    templateId,
  })
  const tools = chatMode === 'chat' ? [] : toolManager.getAllToolDefinitions()

  // å‘é€è¯·æ±‚
  api.llm.send({ config: config as any, messages: messages as any, tools, systemPrompt: '' }).catch(() => {
    processor.cleanup()
  })

  const result = await processor.wait()
  performanceMonitor.end(`llm:${config.model}`, !result.error)

  // æ›´æ–° usage
  if (assistantId && result.usage) {
    useAgentStore.getState().updateMessage(assistantId, { usage: result.usage } as any)
  }

  return result
}

async function callLLMWithRetry(
  config: LLMConfig,
  messages: OpenAIMessage[],
  chatMode: WorkMode,
  assistantId: string | null,
  abortSignal?: AbortSignal
): Promise<LLMCallResult> {
  const retryConfig = getAgentConfig()
  try {
    return await withRetry(
      async () => {
        if (abortSignal?.aborted) throw new Error('Aborted')
        const result = await callLLM(config, messages, chatMode, assistantId)
        if (result.error) throw new Error(result.error)
        return result
      },
      {
        maxRetries: retryConfig.maxRetries,
        initialDelayMs: retryConfig.retryDelayMs,
        backoffMultiplier: retryConfig.retryBackoffMultiplier,
        isRetryable: error => {
          const msg = error instanceof Error ? error.message : String(error)
          return isRetryableError(error) && msg !== 'Aborted'
        },
        onRetry: (attempt, error, delay) => logger.agent.info(`[Loop] LLM retry ${attempt}, waiting ${delay}ms...`, error),
      }
    )
  } catch (error) {
    return { error: error instanceof Error ? error.message : String(error) }
  }
}

// ===== è‡ªåŠ¨ä¿®å¤ =====

async function autoFix(
  toolCalls: any[],
  workspacePath: string,
  assistantId: string | null
): Promise<void> {
  const store = useAgentStore.getState()
  const writeToolCalls = toolCalls.filter(tc => !READ_TOOLS.includes(tc.name))
  if (writeToolCalls.length === 0) return

  const editedFiles = writeToolCalls
    .filter(tc => isFileEditTool(tc.name))
    .map(tc => {
      const path = tc.arguments.path as string
      return pathStartsWith(path, workspacePath) ? path : joinPath(workspacePath, path)
    })
    .filter(path => !path.endsWith('/'))

  if (editedFiles.length === 0) return

  // å¹¶è¡Œæ£€æŸ¥æ‰€æœ‰æ–‡ä»¶çš„ lint é”™è¯¯
  const results = await Promise.all(
    editedFiles.map(async (filePath) => {
      try {
        const result = await toolRegistry.execute('get_lint_errors', { path: filePath }, { workspacePath })
        if (result.success && result.result) {
          const text = result.result.trim()
          if (text && text !== '[]' && text !== 'No diagnostics found') {
            if (/\[error\]/i.test(text) || text.includes('failed to compile') || text.includes('syntax error')) {
              return `File: ${filePath}\n${text}`
            }
          }
        }
      } catch { /* ignore */ }
      return null
    })
  )

  const errors = results.filter((e): e is string => e !== null)

  if (errors.length > 0 && assistantId) {
    store.appendToAssistant(assistantId, `\n\nğŸ” **Auto-check**: Detected ${errors.length} issue(s). Attempting to fix...`)
  }
}

// ===== å‹ç¼©æ£€æŸ¥ä¸å¤„ç† =====

interface CompressionCheckResult {
  level: 0 | 1 | 2 | 3 | 4
  needsHandoff: boolean
}

async function checkAndHandleCompression(
  usage: { input: number; output: number },
  contextLimit: number,
  store: ReturnType<typeof useAgentStore.getState>,
  context: ExecutionContext,
  assistantId: string,
  enableLLMSummary: boolean,
  autoHandoff: boolean
): Promise<CompressionCheckResult> {
  const totalUsed = usage.input + usage.output
  const ratio = totalUsed / contextLimit
  const level = getCompressionLevel(ratio)

  const thread = store.getCurrentThread()
  const userTurns = thread?.messages.filter(m => m.role === 'user').length || 0

  logger.agent.info(`[Compression] Level ${level} (${COMPRESSION_LEVEL_NAMES[level]}), ratio: ${(ratio * 100).toFixed(1)}%, tokens: ${totalUsed}/${contextLimit}`)

  // è®°å½•æœ¬æ¬¡æ–°å¢èŠ‚çœçš„ token æ•°
  let newSavedTokens = 0
  
  // è®¡ç®—å·²ç»å‹ç¼©çš„ token æ•°ï¼ˆä¹‹å‰ prune è¿‡çš„ï¼‰
  let alreadySavedTokens = 0
  if (thread) {
    for (const msg of thread.messages) {
      if (msg.role === 'tool') {
        const toolMsg = msg as import('../types').ToolResultMessage
        if (toolMsg.compactedAt) {
          // å·²å‹ç¼©çš„å·¥å…·ç»“æœï¼Œä¼°ç®—å…¶åŸå§‹å¤§å°
          const content = typeof toolMsg.content === 'string' ? toolMsg.content : ''
          // å¦‚æœå†…å®¹æ˜¯å ä½ç¬¦ï¼Œä¼°ç®—åŸå§‹å¤§å°ï¼ˆå‡è®¾å¹³å‡ 2000 tokenï¼‰
          if (content === '[Old tool result content cleared]' || content.length < 100) {
            alreadySavedTokens += 2000
          } else {
            alreadySavedTokens += estimateTokens(content)
          }
        }
      }
    }
  }

  // L2+: æ‰§è¡Œ prune å¹¶æ ‡è®°å½“å‰ assistant æ¶ˆæ¯ä¸ºå‹ç¼©ç‚¹
  if (level >= 2 && thread) {
    // æ ‡è®°å‹ç¼©ç‚¹
    store.updateMessage(assistantId, { compactedAt: Date.now() } as any)
    
    // æ‰§è¡Œ prune
    const pruneResult = pruneMessages(thread.messages)
    if (pruneResult.prunedCount > 0) {
      for (const msgId of pruneResult.messagesToCompact) {
        store.updateMessage(msgId, { compactedAt: Date.now() } as any)
      }
      newSavedTokens = pruneResult.pruned
      logger.agent.info(`[Compression] Pruned ${pruneResult.prunedCount} tool results, saved ~${pruneResult.pruned} tokens`)
      EventBus.emit({ type: 'context:prune', prunedCount: pruneResult.prunedCount, savedTokens: pruneResult.pruned })
    }
  }
  
  // æ€»èŠ‚çœ = å·²å‹ç¼© + æœ¬æ¬¡æ–°å¢
  const totalSavedTokens = alreadySavedTokens + newSavedTokens

  // L3: ç”Ÿæˆ LLM æ‘˜è¦
  if (level >= 3 && enableLLMSummary && thread) {
    try {
      const summaryResult = await generateSummary(thread.messages, { type: 'detailed' })
      store.setContextSummary({
        objective: summaryResult.objective,
        completedSteps: summaryResult.completedSteps,
        pendingSteps: summaryResult.pendingSteps,
        decisions: [],
        fileChanges: summaryResult.fileChanges,
        errorsAndFixes: [],
        userInstructions: [],
        generatedAt: Date.now(),
        turnRange: [0, userTurns],
      })
      EventBus.emit({ type: 'context:summary', summary: summaryResult.summary })
    } catch {
      // æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
    }
  }

  // L4: ç”Ÿæˆ Handoff æ–‡æ¡£ï¼ˆä»…å½“ autoHandoff å¯ç”¨æ—¶ï¼‰
  if (level >= 4) {
    if (autoHandoff && thread && context.workspacePath) {
      try {
        const handoff = await generateHandoffDocument(thread.id, thread.messages, context.workspacePath)
        store.setHandoffDocument(handoff)
        EventBus.emit({ type: 'context:handoff', document: handoff })
      } catch {
        // Handoff ç”Ÿæˆå¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
      }
    }

    const { language } = useStore.getState()
    const msg = language === 'zh'
      ? 'âš ï¸ **ä¸Šä¸‹æ–‡å·²æ»¡**\n\nå½“å‰å¯¹è¯å·²è¾¾åˆ°ä¸Šä¸‹æ–‡é™åˆ¶ã€‚æˆ‘å·²ä¿å­˜å¯¹è¯æ‘˜è¦ï¼Œæ‚¨å¯ä»¥å¼€å§‹æ–°ä¼šè¯ç»§ç»­ã€‚'
      : 'âš ï¸ **Context Limit Reached**\n\nI have saved a summary of our conversation. Please start a new session to continue.'
    store.appendToAssistant(assistantId, msg)
    store.setHandoffRequired(true)
  }

  // æ›´æ–°å‹ç¼©ç»Ÿè®¡ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„è½®æ¬¡è®¾ç½®ï¼‰
  const agentConfig = getAgentConfig()
  const keptTurns = Math.min(
    userTurns, 
    level === 0 ? userTurns 
      : level === 1 ? agentConfig.keepRecentTurns * 2  // L1: ä¿ç•™æ›´å¤š
      : level === 2 ? agentConfig.keepRecentTurns      // L2: ä½¿ç”¨ keepRecentTurns
      : level === 3 ? agentConfig.deepCompressionTurns // L3: ä½¿ç”¨ deepCompressionTurns
      : agentConfig.deepCompressionTurns               // L4: æœ€å°‘ä¿ç•™
  )
  const compactedTurns = Math.max(0, userTurns - keptTurns)

  // è®¡ç®—åŸå§‹ token æ•°ï¼ˆå½“å‰ä½¿ç”¨ + å·²èŠ‚çœçš„ï¼‰
  const originalTokens = totalUsed + totalSavedTokens
  // å½“å‰ token æ•°å°±æ˜¯ LLM è¿”å›çš„çœŸå®ä½¿ç”¨é‡
  const finalTokens = totalUsed
  // èŠ‚çœç™¾åˆ†æ¯”
  const savedPercent = totalSavedTokens > 0 ? Math.round((totalSavedTokens / originalTokens) * 100) : 0

  store.setCompressionStats({
    level,
    levelName: COMPRESSION_LEVEL_NAMES[level],
    originalTokens,
    finalTokens,
    savedPercent,
    keptTurns,
    compactedTurns,
    needsHandoff: level >= 4,
    lastOptimizedAt: Date.now(),
  })

  if (totalSavedTokens > 0) {
    logger.agent.info(`[Compression] Stats: original=${originalTokens}, final=${finalTokens}, saved=${savedPercent}%`)
  }

  EventBus.emit({ type: 'context:level', level, tokens: totalUsed, ratio })

  return { level, needsHandoff: level >= 4 }
}

// ===== ä¸»å¾ªç¯ =====

export async function runLoop(
  config: LLMConfig,
  llmMessages: OpenAIMessage[],
  context: ExecutionContext,
  assistantId: string
): Promise<void> {
  const store = useAgentStore.getState()
  const mainStore = useStore.getState()
  
  // ä¸€æ¬¡æ€§è·å–æ‰€æœ‰é…ç½®ï¼Œé¿å…é‡å¤è°ƒç”¨ getState()
  const agentConfig = getAgentConfig()
  const maxIterations = mainStore.agentConfig.maxToolLoops || agentConfig.maxToolLoops
  const enableAutoFix = mainStore.agentConfig.enableAutoFix
  const enableLLMSummary = mainStore.agentConfig.enableLLMSummary
  const autoHandoff = mainStore.agentConfig.autoHandoff ?? agentConfig.autoHandoff

  // è·å–æ¨¡å‹ä¸Šä¸‹æ–‡é™åˆ¶ï¼ˆé»˜è®¤ 128kï¼‰
  const contextLimit = config.contextLimit || 128_000

  const loopDetector = new LoopDetector()
  let iteration = 0
  let shouldContinue = true

  EventBus.emit({ type: 'loop:start' })

  while (shouldContinue && iteration < maxIterations && !context.abortSignal?.aborted) {
    iteration++
    shouldContinue = false
    EventBus.emit({ type: 'loop:iteration', count: iteration })

    if (llmMessages.length === 0) {
      logger.agent.error('[Loop] No messages to send')
      store.appendToAssistant(assistantId, '\n\nâŒ Error: No messages to send')
      EventBus.emit({ type: 'loop:end', reason: 'no_messages' })
      break
    }

    // è°ƒç”¨ LLM
    const result = await callLLMWithRetry(config, llmMessages, context.chatMode, assistantId, context.abortSignal)

    if (context.abortSignal?.aborted) {
      EventBus.emit({ type: 'loop:end', reason: 'aborted' })
      break
    }

    if (result.error) {
      logger.agent.error('[Loop] LLM error:', result.error)
      store.appendToAssistant(assistantId, `\n\nâŒ Error: ${result.error}`)
      EventBus.emit({ type: 'loop:end', reason: 'error' })
      break
    }

    // åœ¨ LLM è°ƒç”¨åç«‹å³æ£€æŸ¥å‹ç¼©ï¼ˆå‚è€ƒ OpenCode çš„ finish-step é€»è¾‘ï¼‰
    if (result.usage) {
      const usage = {
        input: result.usage.promptTokens || 0,
        output: result.usage.completionTokens || 0,
      }

      const compressionResult = await checkAndHandleCompression(
        usage,
        contextLimit,
        store,
        context,
        assistantId,
        enableLLMSummary,
        autoHandoff
      )

      // L4 éœ€è¦ä¸­æ–­å¾ªç¯
      if (compressionResult.needsHandoff) {
        EventBus.emit({ type: 'loop:end', reason: 'handoff_required' })
        break
      }
    }

    // æ²¡æœ‰å·¥å…·è°ƒç”¨
    if (!result.toolCalls || result.toolCalls.length === 0) {
      // Plan æ¨¡å¼æé†’
      if (context.chatMode === 'plan' && store.plan) {
        const readOnlyTools = getReadOnlyTools()
        const hasWriteOps = llmMessages.some(m => m.role === 'assistant' && m.tool_calls?.some((tc: any) => !readOnlyTools.includes(tc.function.name)))
        const hasUpdatePlan = llmMessages.some(m => m.role === 'assistant' && m.tool_calls?.some((tc: any) => tc.function.name === 'update_plan'))
        if (hasWriteOps && !hasUpdatePlan && iteration < maxIterations) {
          llmMessages.push({ role: 'user', content: 'Reminder: Please use `update_plan` to update the plan status before finishing.' })
          shouldContinue = true
          continue
        }
      }
      EventBus.emit({ type: 'loop:end', reason: 'complete' })
      break
    }

    // å¾ªç¯æ£€æµ‹
    const loopCheck = loopDetector.checkLoop(result.toolCalls)
    if (loopCheck.isLoop) {
      logger.agent.warn(`[Loop] Loop detected: ${loopCheck.reason}`)
      const suggestion = loopCheck.suggestion ? `\nğŸ’¡ ${loopCheck.suggestion}` : ''
      store.appendToAssistant(assistantId, `\n\nâš ï¸ ${loopCheck.reason}${suggestion}`)
      EventBus.emit({ type: 'loop:warning', message: loopCheck.reason || 'Loop detected' })
      EventBus.emit({ type: 'loop:end', reason: 'loop_detected' })
      break
    }

    // æ·»åŠ å·¥å…·è°ƒç”¨åˆ° UI
    const currentMsg = store.getMessages().find(m => m.id === assistantId)
    if (currentMsg?.role === 'assistant') {
      const existing = (currentMsg as any).toolCalls || []
      for (const tc of result.toolCalls) {
        if (!existing.find((e: any) => e.id === tc.id)) {
          store.addToolCallPart(assistantId, { id: tc.id, name: tc.name, arguments: tc.arguments })
        }
      }
    }

    // æ·»åŠ åˆ°æ¶ˆæ¯å†å²
    llmMessages.push({
      role: 'assistant',
      content: result.content || null,
      tool_calls: result.toolCalls.map(tc => ({
        id: tc.id,
        type: 'function' as const,
        function: { name: tc.name, arguments: JSON.stringify(tc.arguments) },
      })),
    })

    // æ‰§è¡Œå·¥å…·
    const { results: toolResults, userRejected } = await executeTools(
      result.toolCalls,
      { workspacePath: context.workspacePath, currentAssistantId: assistantId },
      context.abortSignal
    )

    // æ£€æŸ¥ ask_user
    const waitingResult = toolResults.find(r => r.result.meta?.waitingForUser)
    if (waitingResult) {
      // ä» meta ä¸­æå– interactive æ•°æ®å¹¶è®¾ç½®åˆ° store
      const interactive = waitingResult.result.meta?.interactive as import('../types').InteractiveContent | undefined
      if (interactive) {
        store.setInteractive(assistantId, interactive)
      } else {
        // å…œåº•ï¼šå¦‚æœæ²¡æœ‰ interactive æ•°æ®ï¼Œè‡³å°‘è¦ finalize
        store.finalizeAssistant(assistantId)
      }
      store.setStreamPhase('idle')
      EventBus.emit({ type: 'loop:end', reason: 'waiting_for_user' })
      break
    }

    // æ·»åŠ å·¥å…·ç»“æœ
    for (const { toolCall, result: toolResult } of toolResults) {
      llmMessages.push({
        role: 'tool' as const,
        tool_call_id: toolCall.id,
        name: toolCall.name,
        content: toolResult.content,
      })
      const meta = toolResult.meta
      if (meta?.filePath && typeof meta.filePath === 'string' && typeof meta.newContent === 'string') {
        loopDetector.updateContentHash(meta.filePath, meta.newContent)
        
        // æ·»åŠ å¾…ç¡®è®¤çš„æ–‡ä»¶å˜æ›´
        store.addPendingChange({
          filePath: meta.filePath,
          toolCallId: toolCall.id,
          toolName: toolCall.name,
          changeType: meta.oldContent ? 'modify' : 'create',
          snapshot: {
            path: meta.filePath,
            content: (meta.oldContent as string) || null,
            timestamp: Date.now(),
          },
          newContent: meta.newContent,
          linesAdded: (meta.linesAdded as number) || 0,
          linesRemoved: (meta.linesRemoved as number) || 0,
        })
      }
    }

    // è‡ªåŠ¨ä¿®å¤ï¼ˆå¹¶è¡Œæ£€æŸ¥ï¼‰
    if (enableAutoFix && !userRejected && context.workspacePath) {
      await autoFix(result.toolCalls, context.workspacePath, assistantId)
    }

    if (userRejected) {
      EventBus.emit({ type: 'loop:end', reason: 'user_rejected' })
      break
    }

    shouldContinue = true
    store.setStreamPhase('streaming')
  }

  if (iteration >= maxIterations) {
    store.appendToAssistant(assistantId, '\n\nâš ï¸ Reached maximum tool call limit.')
    EventBus.emit({ type: 'loop:warning', message: 'Max iterations reached' })
    EventBus.emit({ type: 'loop:end', reason: 'max_iterations' })
  }

  // å¾ªç¯ç»“æŸï¼Œfinalize ç”± Agent.ts çš„ cleanup å¤„ç†
}
